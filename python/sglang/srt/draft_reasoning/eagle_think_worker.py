"""
EAGLE-Think Worker: Main worker class for draft reasoning mode

This module implements the main worker that coordinates between target model
and draft model for the EAGLE-Think reasoning mode.

Copyright 2023-2024 SGLang Team
Licensed under the Apache License, Version 2.0
"""

import logging
from contextlib import contextmanager
from typing import Optional, Tuple

import torch

from sglang.srt.distributed import GroupCoordinator, patch_tensor_parallel_group
from sglang.srt.layers.logits_processor import LogitsProcessorOutput
from sglang.srt.managers.schedule_batch import ScheduleBatch
from sglang.srt.managers.tp_worker import TpModelWorker
from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode, ForwardBatch
from sglang.srt.server_args import ServerArgs
# from sglang.srt.speculative.eagle_utils import fast_topk  # May be used in future extensions
from sglang.srt.draft_reasoning.eagle_think_utils import (
    EagleThinkInput,
    EagleThinkProcessor,
    ThinkingPhase,
    create_thinking_tokens_vocab,
    extract_multi_layer_features,
)

logger = logging.getLogger(__name__)


@contextmanager
def draft_tp_context(tp_group: GroupCoordinator):
    """Context manager for draft model tensor parallel group."""
    with patch_tensor_parallel_group(tp_group):
        yield


class EagleThinkWorker:
    """
    Main worker class for EAGLE-Think mode.
    
    This worker coordinates between the target model and draft model to implement
    the draft reasoning mode where <think>...</think> content is generated by
    the draft model and final reasoning is done by the target model.
    """
    
    def __init__(
        self,
        server_args: ServerArgs,
        target_worker: TpModelWorker,
        draft_worker: TpModelWorker,
        max_thinking_length: int = 512,
    ):
        self.server_args = server_args
        self.target_worker = target_worker
        self.draft_worker = draft_worker
        self.max_thinking_length = max_thinking_length
        self.device = server_args.device
        
        # Initialize thinking tokens
        self.think_start_token_id, self.think_end_token_id = self._initialize_thinking_tokens()
        
        # Initialize the processor
        self.processor = EagleThinkProcessor(
            think_start_token_id=self.think_start_token_id,
            think_end_token_id=self.think_end_token_id,
            max_thinking_length=max_thinking_length,
            device=self.device,
        )
        
        # Current thinking state
        self.current_think_input = EagleThinkInput.create_idle_input(
            device=self.device,
            hidden_size=target_worker.model_runner.model_config.hidden_size,
            dtype=target_worker.model_runner.model_config.dtype,
            max_thinking_length=max_thinking_length,
            think_start_token_id=self.think_start_token_id,
            think_end_token_id=self.think_end_token_id,
        )
        
        logger.info(
            f"EagleThinkWorker initialized with max_thinking_length={max_thinking_length}, "
            f"think_start_token_id={self.think_start_token_id}, "
            f"think_end_token_id={self.think_end_token_id}"
        )
    
    def _initialize_thinking_tokens(self) -> Tuple[Optional[int], Optional[int]]:
        """Initialize thinking tokens from the target model's tokenizer."""
        try:
            # Try to get tokenizer from target worker
            if hasattr(self.target_worker, 'tokenizer'):
                tokenizer = self.target_worker.tokenizer
            elif hasattr(self.target_worker.model_runner, 'tokenizer'):
                tokenizer = self.target_worker.model_runner.tokenizer
            else:
                logger.warning("Could not find tokenizer, using default token IDs")
                return 128000, 128001  # Default placeholder IDs
            
            return create_thinking_tokens_vocab(tokenizer)
        except Exception as e:
            logger.warning(f"Failed to initialize thinking tokens: {e}")
            return 128000, 128001  # Default placeholder IDs
    
    def forward_batch_with_thinking(
        self, 
        batch: ScheduleBatch
    ) -> Tuple[LogitsProcessorOutput, torch.Tensor, bool]:
        """
        Main forward pass with EAGLE-Think reasoning mode.
        
        Args:
            batch: The batch to process
            
        Returns:
            Tuple of (logits_output, next_token_ids, phase_changed)
        """
        # Determine which model to use based on current thinking phase
        if self.processor.should_use_draft_model(self.current_think_input):
            return self._forward_with_draft_model(batch)
        else:
            return self._forward_with_target_model(batch)
    
    def _forward_with_target_model(
        self, 
        batch: ScheduleBatch
    ) -> Tuple[LogitsProcessorOutput, torch.Tensor, bool]:
        """Forward pass using the target model."""
        logger.debug("Using target model for forward pass")
        
        # If transitioning from thinking phase, prepare the batch
        if self.current_think_input.thinking_phase == ThinkingPhase.TRANSITION:
            batch = self.processor.prepare_target_model_input(
                batch, 
                self.current_think_input.get_accumulated_thinking_states()
            )
            # Reset thinking state after transition
            self.current_think_input.reset_thinking_state()
        
        # Forward pass with target model
        logits_output, next_token_ids, _ = self.target_worker.forward_batch_generation(
            batch.get_model_worker_batch()
        )
        
        # Process the generated token
        think_output = self.processor.process_token_generation(
            batch, logits_output, self.current_think_input
        )
        
        # If starting thinking phase, capture hidden states for draft model
        if think_output.phase_changed and think_output.thinking_phase == ThinkingPhase.THINKING:
            self.current_think_input.target_hidden_states = logits_output.hidden_states
        
        return logits_output, think_output.next_token_id, think_output.phase_changed
    
    def _forward_with_draft_model(
        self, 
        batch: ScheduleBatch
    ) -> Tuple[LogitsProcessorOutput, torch.Tensor, bool]:
        """Forward pass using the draft model for thinking content."""
        logger.debug("Using draft model for thinking phase")
        
        # Prepare draft model input with target model's hidden states
        draft_batch = self._prepare_draft_batch(batch)
        
        with draft_tp_context(self.draft_worker.model_runner.tp_group):
            # Forward pass with draft model
            logits_output, _ = self.draft_worker.model_runner.forward(
                draft_batch, skip_attn_backend_init=False
            )
        
        # Process the generated token
        think_output = self.processor.process_token_generation(
            batch, logits_output, self.current_think_input
        )
        
        return logits_output, think_output.next_token_id, think_output.phase_changed
    
    def _prepare_draft_batch(self, batch: ScheduleBatch) -> ForwardBatch:
        """
        Prepare batch for draft model forward pass.
        
        This method adapts the target model's hidden states for the draft model,
        similar to how EAGLE-3 processes features.
        """
        # Extract and fuse multi-layer features from target model
        if self.current_think_input.target_hidden_states is not None:
            fused_features = extract_multi_layer_features(
                self.current_think_input.target_hidden_states
            )
        else:
            # Fallback: create empty features
            hidden_size = self.draft_worker.model_runner.model_config.hidden_size
            fused_features = torch.zeros(
                (batch.batch_size(), hidden_size * 3),
                device=self.device,
                dtype=self.draft_worker.model_runner.model_config.dtype
            )
        
        # Create forward batch for draft model
        model_worker_batch = batch.get_model_worker_batch()
        draft_batch = ForwardBatch.init_new(
            model_worker_batch, self.draft_worker.model_runner
        )
        
        # Set the fused features as spec_info for draft model
        if not hasattr(draft_batch, 'spec_info') or draft_batch.spec_info is None:
            draft_batch.spec_info = EagleThinkInput()
        
        draft_batch.spec_info.target_hidden_states = fused_features
        draft_batch.capture_hidden_mode = CaptureHiddenMode.LAST
        
        return draft_batch
    
    def is_thinking_mode_enabled(self) -> bool:
        """Check if thinking mode is currently enabled."""
        return (
            self.think_start_token_id is not None and 
            self.think_end_token_id is not None
        )
    
    def get_thinking_stats(self) -> dict:
        """Get statistics about the current thinking session."""
        return {
            "thinking_phase": self.current_think_input.thinking_phase.value,
            "thinking_tokens_count": len(self.current_think_input.thinking_tokens),
            "max_thinking_length": self.max_thinking_length,
            "thinking_tokens": self.current_think_input.thinking_tokens.copy(),
        }
    
    def reset_thinking_state(self):
        """Reset the thinking state (useful for new conversations)."""
        self.current_think_input.reset_thinking_state()
        logger.debug("Thinking state reset")
    
    def set_thinking_length_limit(self, max_length: int):
        """Update the maximum thinking length limit."""
        self.max_thinking_length = max_length
        self.current_think_input.max_thinking_length = max_length
        self.processor.max_thinking_length = max_length
        logger.info(f"Updated max thinking length to {max_length}")


class EagleThinkManager:
    """
    Manager class to integrate EAGLE-Think with existing SGLang infrastructure.
    
    This class provides a high-level interface to enable/disable EAGLE-Think mode
    and integrate it with the existing model workers.
    """
    
    def __init__(self, server_args: ServerArgs):
        self.server_args = server_args
        self.eagle_think_worker: Optional[EagleThinkWorker] = None
        self.enabled = False
    
    def initialize(
        self, 
        target_worker: TpModelWorker, 
        draft_worker: Optional[TpModelWorker] = None,
        max_thinking_length: int = 512,
    ):
        """
        Initialize EAGLE-Think with the provided workers.
        
        Args:
            target_worker: The main target model worker
            draft_worker: The draft model worker (if None, will try to use EAGLE draft model)
            max_thinking_length: Maximum length for thinking sequences
        """
        if draft_worker is None:
            logger.warning("No draft worker provided, EAGLE-Think mode will be disabled")
            return False
        
        try:
            self.eagle_think_worker = EagleThinkWorker(
                server_args=self.server_args,
                target_worker=target_worker,
                draft_worker=draft_worker,
                max_thinking_length=max_thinking_length,
            )
            self.enabled = True
            logger.info("EAGLE-Think mode initialized successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize EAGLE-Think mode: {e}")
            return False
    
    def is_enabled(self) -> bool:
        """Check if EAGLE-Think mode is enabled."""
        return self.enabled and self.eagle_think_worker is not None
    
    def forward_batch(self, batch: ScheduleBatch) -> Tuple[LogitsProcessorOutput, torch.Tensor, bool]:
        """
        Forward batch through EAGLE-Think if enabled, otherwise use normal processing.
        
        Args:
            batch: The batch to process
            
        Returns:
            Tuple of (logits_output, next_token_ids, phase_changed)
        """
        if not self.is_enabled():
            raise RuntimeError("EAGLE-Think mode is not enabled")
        
        return self.eagle_think_worker.forward_batch_with_thinking(batch)
    
    def get_stats(self) -> dict:
        """Get statistics about EAGLE-Think processing."""
        if not self.is_enabled():
            return {"enabled": False}
        
        stats = self.eagle_think_worker.get_thinking_stats()
        stats["enabled"] = True
        return stats
    
    def reset_state(self):
        """Reset EAGLE-Think state."""
        if self.is_enabled():
            self.eagle_think_worker.reset_thinking_state()
